{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59e585b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to C:\\Users\\Cheng\n",
      "[nltk_data]     An\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import re\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "from textblob import TextBlob\n",
    "\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da05e5c6",
   "metadata": {},
   "source": [
    "# SNSCRAPE to scrape through Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8c9221f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ToneVays\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CHENGA~1\\AppData\\Local\\Temp/ipykernel_38816/2003446335.py:22: FutureWarning: username is deprecated, use user.username instead\n",
      "  tweets_list.append([tweet.date, tweet.id, tweet.content, tweet.username])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "officialmcafee\n",
      "\n",
      "VitalikButerin\n",
      "\n",
      "aantop\n",
      "\n",
      "TimDraper\n",
      "\n",
      "rogerkver\n",
      "\n",
      "elonmusk\n",
      "\n",
      "CathieDWood\n",
      "\n",
      "jack\n",
      "\n",
      "michael_saylor\n",
      "\n",
      "CobraBitcoin\n",
      "\n",
      "ErikVoorhees\n",
      "\n",
      "Bitboy_Crypto\n",
      "\n",
      "VinnyLingham\n",
      "\n",
      "adam3us\n",
      "\n",
      "gavinandresen\n",
      "\n",
      "NickSzabo4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Creating list to append tweet data\n",
    "tweets_list = []\n",
    "#keyword\n",
    "keyword = 'bitcoin'\n",
    "# No of tweets\n",
    "noOfTweet = 50000\n",
    "\n",
    "\n",
    "#Loop through the usernames:\n",
    "\n",
    "#user_names = open('News_Station.txt','r')\n",
    "user_names = open('Influential_People.txt','r')\n",
    "\n",
    "for user in user_names:\n",
    "    print (user)\n",
    "    \n",
    "# Using TwitterSearchScraper to scrape data and append tweets to list\n",
    "    for i,tweet in enumerate(sntwitter.TwitterSearchScraper(\"from:\"+ user +\" \"+keyword ).get_items()):\n",
    "        if i > int(noOfTweet):\n",
    "            break\n",
    "        \n",
    "        tweets_list.append([tweet.date, tweet.id, tweet.content, tweet.username])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec302f8",
   "metadata": {},
   "source": [
    "# Creating and cleaning the dataframe from the tweets list above "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7ad0b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe from the tweets list above \n",
    "df = pd.DataFrame(tweets_list, columns=['Datetime', 'Tweet Id', 'Text', 'Username'])\n",
    "df['Datetime'] = pd.to_datetime(df['Datetime'],unit='ms').dt.tz_convert('Asia/Singapore')\n",
    "df['Datetime'] = df['Datetime'].apply(lambda a: datetime.datetime.strftime(a,\"%d-%m-%Y %H:%M:%S\"))\n",
    "df['Datetime'] = pd.to_datetime(df['Datetime'])          \n",
    "df['Tweet Id'] = ('\"'+ df['Tweet Id'].astype(str) + '\"')      \n",
    "\n",
    "# Create a function to clean the tweets\n",
    "def cleanTxt(text):\n",
    "    text = re.sub('@[A-Za-z0â€“9]+', '', text) #Removing @mentions\n",
    "    text = re.sub('#', '', text) # Removing '#' hash tag\n",
    "    text = re.sub('RT[\\s]+', '', text) # Removing RT\n",
    "    text = re.sub('https?:\\/\\/\\S+', '', text) # Removing hyperlink\n",
    "    return text\n",
    "\n",
    "df[\"Text\"] = df[\"Text\"].apply(cleanTxt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227674e3",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16a088ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentiment Analysis\n",
    "def percentage(part,whole):\n",
    "    return 100 * float(part)/float(whole)\n",
    "\n",
    "#Iterating over the tweets in the dataframe\n",
    "\n",
    "def apply_analysis(tweet):\n",
    "    return SentimentIntensityAnalyzer().polarity_scores(tweet)\n",
    "\n",
    "\n",
    "df[['neg','neu','pos','compound']] = df['Text'].apply(apply_analysis).apply(pd.Series)\n",
    "\n",
    "def sentimental_analysis(df):\n",
    "    if df['neg'] > df['pos']:\n",
    "        return 'Negative'\n",
    "    elif df['pos'] > df['neg']:\n",
    "        return 'Positive'\n",
    "    elif df['pos'] == df['neg']:\n",
    "        return 'Neutral'\n",
    "\n",
    "df['Sentiment_NLTK'] = df.apply(sentimental_analysis, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3085006",
   "metadata": {},
   "source": [
    "Textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b15d9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSubjectivity(twt):\n",
    "    return TextBlob(twt).sentiment.subjectivity\n",
    "def getPolarity(twt):\n",
    "    return TextBlob(twt).sentiment.polarity\n",
    "def getSentiment(score):\n",
    "    if score<0:\n",
    "        return 'Negative'\n",
    "    elif score==0:\n",
    "        return 'Neutral'\n",
    "    else:\n",
    "        return 'Positive'\n",
    "    \n",
    "df['Subjectivity']=df['Text'].apply(getSubjectivity)\n",
    "df['Polarity']=df['Text'].apply(getPolarity)    \n",
    "df['Sentiment_TB']=df['Polarity'].apply(getSentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d1822b",
   "metadata": {},
   "source": [
    "# Generating csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5dcbc957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "#df.to_csv('News_Station.csv', encoding='utf-8-sig')\n",
    "df.to_csv('Influential_People.csv', encoding='utf-8-sig' ,index= False)\n",
    "\n",
    "print('Done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
